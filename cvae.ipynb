{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建VAE模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense,Layer,Input,Lambda\n",
    "from keras.layers import Input, Concatenate, Add\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import manifold\n",
    "from scipy.stats import wasserstein_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算欧氏距离\n",
    "# def dist(x,y):\n",
    "#     return np.sqrt(np.sum(np.square(x-y)))\n",
    "\n",
    "\n",
    "# # 按欧式距离计算精确度\n",
    "# def calAccuracy_Euc(latent_x,latent_c,class_num = 3):\n",
    "#     predict_list = []\n",
    "# #     latent_x = latent_x.tolist()\n",
    "# #     latent_c = latent_c.tolist()\n",
    "    \n",
    "#     if class_num == 3:\n",
    "#         for i in range(6000): \n",
    "#             distance_list = [dist(latent_x[i], latent_c[0]),\n",
    "#                              dist(latent_x[i], latent_c[1]),\n",
    "#                              dist(latent_x[i], latent_c[2])]\n",
    "#             min_index = distance_list.index(min(distance_list))\n",
    "#             predict_list.append(min_index)\n",
    "        \n",
    "# #         print(predict_list)\n",
    "\n",
    "#         count = 0\n",
    "#         for i in range(0,2000):\n",
    "#             if predict_list[i]==0:\n",
    "#                 count=count+1\n",
    "#         for i in range(2000,4000):\n",
    "#             if predict_list[i]==1:\n",
    "#                 count=count+1\n",
    "#         for i in range(4000,6000):\n",
    "#             if predict_list[i]==2:\n",
    "#                 count=count+1\n",
    "\n",
    "#         return count\n",
    "    \n",
    "#     if class_num == 4:\n",
    "#         for i in range(8000):\n",
    "#             distance_list = [dist(latent_x[i], latent_c[0]),\n",
    "#                              dist(latent_x[i], latent_c[1]),\n",
    "#                              dist(latent_x[i], latent_c[2]),\n",
    "#                              dist(latent_x[i], latent_c[3])]\n",
    "#             min_index = distance_list.index(min(distance_list))\n",
    "#             predict_list.append(min_index)\n",
    "        \n",
    "# #         print(predict_list)\n",
    "\n",
    "#         count = 0\n",
    "#         for i in range(0,2000):\n",
    "#             if predict_list[i]==0:\n",
    "#                 count=count+1\n",
    "#         for i in range(2000,4000):\n",
    "#             if predict_list[i]==1:\n",
    "#                 count=count+1\n",
    "#         for i in range(4000,6000):\n",
    "#             if predict_list[i]==2:\n",
    "#                 count=count+1\n",
    "#         for i in range(6000,8000):\n",
    "#             if predict_list[i]==3:\n",
    "#                 count=count+1\n",
    "#         return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算余弦距离\n",
    "# def dist(vector_a, vector_b):\n",
    "#     \"\"\"\n",
    "#     计算两个向量之间的余弦相似度\n",
    "#     :param vector_a: 向量 a \n",
    "#     :param vector_b: 向量 b\n",
    "#     :return: sim\n",
    "#     \"\"\"\n",
    "#     vector_a = np.mat(vector_a)\n",
    "#     vector_b = np.mat(vector_b)\n",
    "#     num = float(vector_a * vector_b.T)\n",
    "#     denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "#     cos = num / denom\n",
    "#     sim = 0.5 + 0.5 * cos\n",
    "#     return sim\n",
    "\n",
    "# 曼哈顿距离\n",
    "def dist(x,y):\n",
    "    return np.sum(np.abs(x - y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 按欧式距离计算精确度\n",
    "def calAccuracy_Euc(latent_x,latent_c,class_num = 3):\n",
    "    predict_list = []\n",
    "#     latent_x = latent_x.tolist()\n",
    "#     latent_c = latent_c.tolist()\n",
    "    \n",
    "    if class_num == 3:\n",
    "        for i in range(6000): \n",
    "            distance_list = [dist(latent_x[i], latent_c[0]),\n",
    "                             dist(latent_x[i], latent_c[1]),\n",
    "                             dist(latent_x[i], latent_c[2])]\n",
    "            min_index = distance_list.index(min(distance_list))\n",
    "            predict_list.append(min_index)\n",
    "        \n",
    "#         print(predict_list)\n",
    "\n",
    "        count = 0\n",
    "        for i in range(0,2000):\n",
    "            if predict_list[i]==0:\n",
    "                count=count+1\n",
    "        for i in range(2000,4000):\n",
    "            if predict_list[i]==1:\n",
    "                count=count+1\n",
    "        for i in range(4000,6000):\n",
    "            if predict_list[i]==2:\n",
    "                count=count+1\n",
    "\n",
    "        return count\n",
    "    \n",
    "    if class_num == 4:\n",
    "        for i in range(8000):\n",
    "            distance_list = [dist(latent_x[i], latent_c[0]),\n",
    "                             dist(latent_x[i], latent_c[1]),\n",
    "                             dist(latent_x[i], latent_c[2]),\n",
    "                             dist(latent_x[i], latent_c[3])]\n",
    "            min_index = distance_list.index(min(distance_list))\n",
    "            predict_list.append(min_index)\n",
    "        \n",
    "#         print(predict_list)\n",
    "\n",
    "        count = 0\n",
    "        for i in range(0,2000):\n",
    "            if predict_list[i]==0:\n",
    "                count=count+1\n",
    "        for i in range(2000,4000):\n",
    "            if predict_list[i]==1:\n",
    "                count=count+1\n",
    "        for i in range(4000,6000):\n",
    "            if predict_list[i]==2:\n",
    "                count=count+1\n",
    "        for i in range(6000,8000):\n",
    "            if predict_list[i]==3:\n",
    "                count=count+1\n",
    "        return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集视觉特征\n",
    "x_train_img = np.load(r\".\\data\\train\\x_train_2048.npy\")\n",
    "y_train_oh = np.load(r\".\\data\\train\\y_train_oh.npy\")\n",
    "# 测试集视觉特征\n",
    "x_test_img = np.load(r\".\\data\\test\\x_test_2048.npy\")\n",
    "y_test_oh = np.load(r\".\\data\\test\\y_test_oh.npy\")\n",
    "# 训练集语义特征\n",
    "aux_data_train = np.load(r\"./data/aux_data/aux_data_train_lsk.npy\")\n",
    "# 测试集语义特征\n",
    "aux_data_test = np.load(r\"./data/aux_data/aux_data_test_lsk.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.90674120e-01 1.29247099e-01 1.34377941e-01 8.72726887e-02\n",
      " 3.12360376e-02 1.47093743e-01 1.57519728e-01 1.24866650e-01\n",
      " 1.43989474e-01 1.08339876e-01 9.95441377e-02 1.30693525e-01\n",
      " 1.32688135e-01 1.61114082e-01 1.02249168e-01 2.15357572e-01\n",
      " 0.00000000e+00 5.74671179e-02 5.81233315e-02 5.34030013e-02\n",
      " 5.09022921e-03 1.51421539e-02 7.85135105e-02 3.30955498e-02\n",
      " 2.77238768e-02 7.82911852e-02 2.06232797e-02 5.71357533e-02\n",
      " 3.54123227e-02 5.30562811e-02 3.25555131e-02 1.43093690e-01\n",
      " 0.00000000e+00 1.35395974e-02 1.92053095e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.65927795e-02 1.75201930e-02 0.00000000e+00 3.24675604e-03\n",
      " 0.00000000e+00 2.17376519e-02 1.56021006e-02 3.36774401e-02\n",
      " 4.59270775e-02 3.01755760e-02 3.01536079e-02 2.01799590e-02\n",
      " 4.29205038e-02 1.12923237e-06 5.66286333e-02 0.00000000e+00\n",
      " 0.00000000e+00 5.14874384e-02 1.18363230e-02 5.58736362e-02\n",
      " 6.90748692e-02 0.00000000e+00 6.31176820e-03 3.07601355e-02\n",
      " 5.50556220e-02 4.52550873e-02 5.37536517e-02 6.00618869e-02\n",
      " 0.00000000e+00 4.85018007e-02 4.16052043e-02 7.34911710e-02\n",
      " 3.85933109e-02 4.93521802e-02 3.75326835e-02 0.00000000e+00\n",
      " 7.90930390e-02 9.03191343e-02 2.78018285e-02 6.42070994e-02\n",
      " 1.04914606e-01 6.03171214e-02 6.75145686e-02 3.54107991e-02\n",
      " 6.37941882e-02 3.65565047e-02 5.41809648e-02 5.05433977e-02\n",
      " 4.56922315e-02 4.81568426e-02 7.68788978e-02 4.20075133e-02\n",
      " 2.45636310e-02 4.52352762e-02 4.63288538e-02 1.13003142e-01\n",
      " 1.00042172e-01 9.03891698e-02 6.08231723e-02 8.97009820e-02\n",
      " 6.10451922e-02 8.04805532e-02 5.15721627e-02 6.94881529e-02\n",
      " 4.83844131e-02 7.36032277e-02 6.02089092e-02 5.25227673e-02\n",
      " 6.70304671e-02 9.25852060e-02 8.02879184e-02 6.98166341e-02\n",
      " 9.62057784e-02 9.50696766e-02 7.67721310e-02 0.00000000e+00\n",
      " 8.40085074e-02 7.73177892e-02 7.78492093e-02 4.25959565e-02\n",
      " 4.67493795e-02 1.06024504e-01 1.27767831e-01 0.00000000e+00\n",
      " 0.00000000e+00 7.52610043e-02 8.51806551e-02 1.04690120e-01\n",
      " 6.17087409e-02 8.79741088e-02 3.43776401e-03 6.44639581e-02\n",
      " 6.98168874e-02 1.03695029e-02 2.22113859e-02 5.87678105e-02\n",
      " 5.72722666e-02 2.99888737e-02 0.00000000e+00 4.39981557e-02\n",
      " 6.94521219e-02 6.97165057e-02 4.57384959e-02 5.93275055e-02\n",
      " 5.16952984e-02 2.51747742e-02 1.76885482e-02 3.92065793e-02\n",
      " 7.27087706e-02 3.21536511e-02 0.00000000e+00 2.70234738e-02\n",
      " 5.51913232e-02 4.72883172e-02 1.97358541e-02 1.35391476e-02\n",
      " 3.09716482e-02 0.00000000e+00 6.53019100e-02 4.56039608e-02\n",
      " 1.77234579e-02 6.20283373e-02 8.47105961e-03 4.43806313e-02\n",
      " 0.00000000e+00 5.72405010e-02 0.00000000e+00 0.00000000e+00\n",
      " 5.88750467e-02 0.00000000e+00 2.64446274e-03 5.22871017e-02\n",
      " 4.06677835e-02 0.00000000e+00 0.00000000e+00 3.02581973e-02\n",
      " 3.72359045e-02 3.10098608e-06 0.00000000e+00 5.50999455e-02\n",
      " 0.00000000e+00 3.72197339e-03 3.56190763e-02 3.22355703e-02\n",
      " 4.57798094e-02 7.97525421e-03 0.00000000e+00 2.16706824e-02\n",
      " 2.95943934e-02 1.28688058e-02 5.43977395e-02 1.28153414e-02\n",
      " 5.20800017e-02 6.05056025e-02 2.38156226e-02 0.00000000e+00\n",
      " 2.35110801e-03 4.06468213e-02 5.72585985e-02 0.00000000e+00\n",
      " 0.00000000e+00 5.30843511e-02 0.00000000e+00 4.55602109e-02\n",
      " 1.41411703e-02 6.77866652e-08 8.21641274e-03 5.46768643e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.36670648e-02\n",
      " 4.38195840e-02 0.00000000e+00 3.43615860e-02 0.00000000e+00\n",
      " 5.36655762e-07 5.90340085e-02 0.00000000e+00 5.32722822e-07\n",
      " 5.41039594e-02 0.00000000e+00 5.27310781e-02 2.87903287e-02\n",
      " 6.02121167e-02 4.89480011e-02 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.60181781e-02 5.41821048e-02\n",
      " 3.22537795e-02 0.00000000e+00 2.59274710e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.89732125e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.00822845e-02\n",
      " 0.00000000e+00 0.00000000e+00 5.16353920e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.51183638e-02 5.92125393e-02\n",
      " 2.62363756e-04 0.00000000e+00 0.00000000e+00 7.22511932e-02]\n"
     ]
    }
   ],
   "source": [
    "print(aux_data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# IF = [[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]]*2000\n",
    "# OF = [[0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0]]*2000\n",
    "# BF = [[0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1]]*2000\n",
    "# IO = [[1,1,1,1,1,1,1,1,1,0,1,0,1,0,1,0]]*2000\n",
    "# IB = [[1,1,1,0,0,0,0,1,0,0,0,1,1,0,1,1]]*2000\n",
    "# OB = [[0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0]]*2000\n",
    "# IOB =[[1,1,1,1,1,1,0,1,1,0,1,0,0,0,1,1]]*2000\n",
    "\n",
    "# aux_data_train = IF + OF +BF\n",
    "# aux_data_test = IO + IB + OB + IOB\n",
    "\n",
    "# aux_data_train = np.array(aux_data_train)\n",
    "# print(aux_data_train.shape)\n",
    "\n",
    "# aux_data_test = np.array(aux_data_test)\n",
    "# print(aux_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 5.235015\n",
      "accuracy is 25.00%\n",
      "accuracy is 33.33%\n",
      "train loss 1.1493754\n",
      "accuracy is 54.80%\n",
      "accuracy is 75.43%\n",
      "train loss 0.41331047\n",
      "accuracy is 54.86%\n",
      "accuracy is 76.00%\n",
      "train loss 0.22598054\n",
      "accuracy is 54.89%\n",
      "accuracy is 76.78%\n",
      "train loss 0.15526776\n",
      "accuracy is 54.84%\n",
      "accuracy is 76.37%\n",
      "train loss 0.13547933\n",
      "accuracy is 53.96%\n",
      "accuracy is 76.28%\n",
      "train loss 0.11853818\n",
      "accuracy is 54.48%\n",
      "accuracy is 76.28%\n",
      "train loss 0.113097325\n",
      "accuracy is 53.23%\n",
      "accuracy is 76.33%\n",
      "train loss 0.09413569\n",
      "accuracy is 54.71%\n",
      "accuracy is 76.70%\n",
      "train loss 0.09650582\n",
      "accuracy is 55.35%\n",
      "accuracy is 76.70%\n",
      "train loss 0.08670704\n",
      "accuracy is 55.16%\n",
      "accuracy is 76.83%\n",
      "train loss 0.09372687\n",
      "accuracy is 55.42%\n",
      "accuracy is 76.78%\n",
      "train loss 0.09168118\n",
      "accuracy is 54.26%\n",
      "accuracy is 76.28%\n",
      "train loss 0.08261253\n",
      "accuracy is 55.42%\n",
      "accuracy is 76.92%\n",
      "train loss 0.08138883\n",
      "accuracy is 55.54%\n",
      "accuracy is 77.00%\n",
      "train loss 0.07642102\n",
      "accuracy is 53.27%\n",
      "accuracy is 75.25%\n",
      "train loss 0.07129581\n",
      "accuracy is 54.65%\n",
      "accuracy is 77.08%\n",
      "train loss 0.075426705\n",
      "accuracy is 54.45%\n",
      "accuracy is 76.70%\n",
      "train loss 0.075194106\n",
      "accuracy is 55.17%\n",
      "accuracy is 77.40%\n",
      "train loss 0.07216385\n",
      "accuracy is 55.55%\n",
      "accuracy is 76.85%\n",
      "train loss 0.076413974\n",
      "accuracy is 55.64%\n",
      "accuracy is 76.75%\n",
      "train loss 0.07200242\n",
      "accuracy is 55.06%\n",
      "accuracy is 76.87%\n",
      "train loss 0.075167745\n",
      "accuracy is 54.65%\n",
      "accuracy is 76.43%\n",
      "train loss 0.07221221\n",
      "accuracy is 54.33%\n",
      "accuracy is 76.98%\n",
      "train loss 0.07543874\n",
      "accuracy is 55.91%\n",
      "accuracy is 76.80%\n",
      "train loss 0.07617053\n",
      "accuracy is 52.86%\n",
      "accuracy is 74.60%\n",
      "train loss 0.07458682\n",
      "accuracy is 53.01%\n",
      "accuracy is 75.42%\n",
      "train loss 0.0839799\n",
      "accuracy is 55.26%\n",
      "accuracy is 76.33%\n",
      "train loss 0.07461262\n",
      "accuracy is 55.06%\n",
      "accuracy is 77.37%\n",
      "train loss 0.07068464\n",
      "accuracy is 52.67%\n",
      "accuracy is 74.83%\n",
      "train loss 0.07590611\n",
      "accuracy is 52.55%\n",
      "accuracy is 74.43%\n",
      "train loss 0.07169801\n",
      "accuracy is 55.21%\n",
      "accuracy is 77.15%\n",
      "train loss 0.077694625\n",
      "accuracy is 54.06%\n",
      "accuracy is 76.37%\n",
      "train loss 0.07304853\n",
      "accuracy is 53.66%\n",
      "accuracy is 76.40%\n",
      "train loss 0.076885454\n",
      "accuracy is 52.70%\n",
      "accuracy is 74.23%\n",
      "train loss 0.076579064\n",
      "accuracy is 53.71%\n",
      "accuracy is 76.25%\n",
      "train loss 0.079436675\n",
      "accuracy is 53.39%\n",
      "accuracy is 75.35%\n",
      "train loss 0.0703723\n",
      "accuracy is 54.91%\n",
      "accuracy is 76.73%\n",
      "train loss 0.0698449\n",
      "accuracy is 53.01%\n",
      "accuracy is 74.77%\n",
      "train loss 0.081614815\n",
      "accuracy is 53.52%\n",
      "accuracy is 76.10%\n",
      "train loss 0.07764518\n",
      "accuracy is 55.27%\n",
      "accuracy is 76.70%\n",
      "train loss 0.07052972\n",
      "accuracy is 55.24%\n",
      "accuracy is 76.70%\n",
      "train loss 0.072672874\n",
      "accuracy is 52.48%\n",
      "accuracy is 74.90%\n",
      "train loss 0.074562505\n",
      "accuracy is 55.45%\n",
      "accuracy is 76.68%\n",
      "train loss 0.07620504\n",
      "accuracy is 53.81%\n",
      "accuracy is 75.40%\n",
      "train loss 0.07734658\n",
      "accuracy is 55.33%\n",
      "accuracy is 76.83%\n",
      "train loss 0.07189413\n",
      "accuracy is 55.40%\n",
      "accuracy is 77.18%\n",
      "train loss 0.07296875\n",
      "accuracy is 54.29%\n",
      "accuracy is 77.05%\n",
      "train loss 0.078340456\n",
      "accuracy is 54.29%\n",
      "accuracy is 77.20%\n",
      "train loss 0.07219727\n",
      "accuracy is 52.64%\n",
      "accuracy is 76.03%\n",
      "train loss 0.07116476\n",
      "accuracy is 53.62%\n",
      "accuracy is 74.95%\n",
      "train loss 0.06975427\n",
      "accuracy is 54.86%\n",
      "accuracy is 77.13%\n",
      "train loss 0.07098975\n",
      "accuracy is 54.98%\n",
      "accuracy is 76.97%\n",
      "train loss 0.07014063\n",
      "accuracy is 53.50%\n",
      "accuracy is 75.18%\n",
      "train loss 0.07684737\n",
      "accuracy is 55.30%\n",
      "accuracy is 77.12%\n",
      "train loss 0.076022334\n",
      "accuracy is 54.35%\n",
      "accuracy is 76.20%\n",
      "train loss 0.083636045\n",
      "accuracy is 54.71%\n",
      "accuracy is 77.13%\n",
      "train loss 0.07600193\n",
      "accuracy is 55.23%\n",
      "accuracy is 76.95%\n",
      "train loss 0.073165976\n",
      "accuracy is 55.59%\n",
      "accuracy is 76.78%\n",
      "train loss 0.07041025\n",
      "accuracy is 53.56%\n",
      "accuracy is 75.25%\n",
      "train loss 0.077861965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a32c89ae18ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;31m#         print(S_attr_2.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mcount_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalAccuracy_Euc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_img\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS_attr_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mcount_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalAccuracy_Euc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_img\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS_attr_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy is %0.2f%%'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcount_1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy is %0.2f%%'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcount_2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m6000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3312de225fed>\u001b[0m in \u001b[0;36mcalAccuracy_Euc\u001b[1;34m(latent_x, latent_c, class_num)\u001b[0m\n\u001b[0;32m     32\u001b[0m             distance_list = [dist(latent_x[i], latent_c[0]),\n\u001b[0;32m     33\u001b[0m                              \u001b[0mdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_c\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                              dist(latent_x[i], latent_c[2])]\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mmin_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mpredict_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3312de225fed>\u001b[0m in \u001b[0;36mdist\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# 曼哈顿距离\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 语义嵌入视觉(autoencoder)\n",
    "epochs = 10000\n",
    "s_input = 256\n",
    "v_input = 2048\n",
    "hid = 1024\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "\n",
    "    initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "\n",
    "    initial = tf.constant(0.05, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Placeholder\n",
    "# define placeholder for inputs to network\n",
    "v_features = tf.placeholder(tf.float32, [None, v_input])\n",
    "s_features = tf.placeholder(tf.float32, [None, s_input])\n",
    "\n",
    "# # Network\n",
    "# # encoder\n",
    "W_left_a1 = weight_variable([v_input, hid])\n",
    "b_left_a1 = bias_variable([hid])\n",
    "left_a1 = tf.nn.relu(tf.matmul(v_features, W_left_a1) + b_left_a1) # 2048*(2048,1024)-->1024\n",
    "\n",
    "\n",
    "W_left_a2 = weight_variable([hid, s_input])\n",
    "b_left_a2 = bias_variable([s_input])\n",
    "left_a2 = tf.nn.relu(tf.matmul(left_a1, W_left_a2) + b_left_a2) # 1024*(1024,256)-->256\n",
    "\n",
    "## decoder\n",
    "W_left_a3 = weight_variable([s_input, hid])\n",
    "b_left_a3 = bias_variable([hid])\n",
    "left_a3_z = tf.nn.relu(tf.matmul(left_a2, W_left_a3) + b_left_a3) # 256*(256,1024)-->1024\n",
    "left_a3_s = tf.nn.relu(tf.matmul(s_features, W_left_a3) + b_left_a3)  # 256*(256,1024)-->1024\n",
    "\n",
    "W_left_a4 = weight_variable([hid, v_input])\n",
    "b_left_a4 = bias_variable([v_input])\n",
    "left_a4_z = tf.nn.relu(tf.matmul(left_a3_z, W_left_a4) + b_left_a4) # 1024*(1024,2048)-->2048\n",
    "left_a4_s = tf.nn.relu(tf.matmul(left_a3_s, W_left_a4) + b_left_a4) # 1024*(1024,2048)-->2048\n",
    "\n",
    "# # loss\n",
    "\n",
    "# loss_1\n",
    "loss_1 = tf.losses.huber_loss(s_features,left_a2,delta=1.5)\n",
    "hubers_loss = tf.reduce_sum(loss_1)\n",
    "# loss_2\n",
    "loss_2 = tf.losses.absolute_difference(v_features, left_a4_z)\n",
    "loss_2 = tf.reduce_sum(loss_2)\n",
    "# loss_3\n",
    "loss_3 = tf.losses.absolute_difference(v_features, left_a4_s)\n",
    "loss_3 = tf.reduce_sum(loss_3)\n",
    "# # loss_2\n",
    "# loss_2 = tf.losses.mean_squared_error(v_features, left_a4_z)\n",
    "# loss_2 = tf.reduce_sum(loss_2)\n",
    "# # loss_3\n",
    "# loss_3 = tf.losses.mean_squared_error(v_features, left_a4_s)\n",
    "# loss_3 = tf.reduce_sum(loss_3)\n",
    "\n",
    "\n",
    "# L2 regularisation for the fully connected parameters.\n",
    "\n",
    "regularisers_a = (tf.nn.l2_loss(W_left_a1) + tf.nn.l2_loss(b_left_a1)\n",
    "                  + tf.nn.l2_loss(W_left_a2) + tf.nn.l2_loss(b_left_a2)\n",
    "                 +tf.nn.l2_loss(W_left_a3) + tf.nn.l2_loss(b_left_a3)\n",
    "                  + tf.nn.l2_loss(W_left_a4) + tf.nn.l2_loss(b_left_a4))\n",
    "\n",
    "m=0.1\n",
    "n=0.3\n",
    "loss_sum = loss_1+m*loss_2+n*loss_3+1e-3 * regularisers_a\n",
    "\n",
    "#acc=  accuracy()\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.0005).minimize(loss_sum)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# fig_loss = np.zeros([10])\n",
    "# fig_acc = np.zeros([10])\n",
    "#data_fit()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    s=[]\n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, x_train_img.shape[0], 100)\n",
    "\n",
    "    v_feature = x_train_img[idx]\n",
    "    s = aux_data_train[idx]\n",
    "#     for i in range(len(idx)):\n",
    "\n",
    "#         if idx[i] <2000:\n",
    "#             s_attr = aux_data_train[0]\n",
    "#             s.append(s_attr)\n",
    "#         if 2000<= idx[i] < 4000:\n",
    "#             s_attr = aux_data_train[2000]\n",
    "#             s.append(s_attr)\n",
    "#         if 4000<= idx[i] < 6000:\n",
    "#             s_attr = aux_data_train[4000]\n",
    "#             s.append(s_attr)\n",
    "\n",
    "#     s_attr = np.atleast_2d(s_attr)\n",
    "\n",
    "    _, loss_val= sess.run([train_step, loss_sum], feed_dict={v_features: v_feature , s_features: s})\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print('train loss',loss_val)\n",
    "        # 7种语义嵌入子空间\n",
    "        s_attr_1 = np.array([aux_data_train[0],aux_data_train[2000],aux_data_train[4000]])\n",
    "        S_attr_1= sess.run(left_a4_z, feed_dict={left_a2: s_attr_1})\n",
    "#         print(S_attr_1.shape)\n",
    "        s_attr_2 = np.array([aux_data_test[0],aux_data_test[2000],aux_data_test[4000],aux_data_test[6000]])\n",
    "        S_attr_2= sess.run(left_a4_z, feed_dict={left_a2: s_attr_2})\n",
    "#         print(S_attr_2.shape)\n",
    "        count_1 = calAccuracy_Euc(x_test_img,S_attr_2,class_num = 4)\n",
    "        count_2 = calAccuracy_Euc(x_test_img[0:6000],S_attr_2[0:3],class_num = 3)\n",
    "        print('accuracy is %0.2f%%'%(100*count_1/8000))     \n",
    "        print('accuracy is %0.2f%%'%(100*count_2/6000))  \n",
    "\n",
    "   # print('accuracy:',accuracy)\n",
    "#                 if epoch %1000 == 0:\n",
    "#                     acc= accuracy()\n",
    "#                     fig_loss[epoch//1000] = loss_val\n",
    "#                     fig_acc[epoch//1000] = acc\n",
    "\n",
    "\n",
    "# print(acc)                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7种语义嵌入子空间\n",
    "s_attr_1 = np.array([aux_data_train[0],aux_data_train[2000],aux_data_train[4000]])\n",
    "S_attr_1= sess.run(left_a2, feed_dict={att_features: s_attr_1})\n",
    "print(S_attr_1.shape)\n",
    "\n",
    "s_attr_2 = np.array([aux_data_test[0],aux_data_test[2000],aux_data_test[4000],aux_data_test[6000]])\n",
    "S_attr_2= sess.run(left_a2, feed_dict={att_features: s_attr_2})\n",
    "print(S_attr_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集精确度\n",
    "count = calAccuracy_Euc(x_train_img,S_attr_1)\n",
    "print('accuracy is %0.2f%%'%(100*count/6000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算测试集精确度-4class\n",
    "count = calAccuracy_Euc(x_test_img,S_attr_2,class_num = 4)\n",
    "print('accuracy is %0.2f%%'%(100*count/8000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算测试集精确度-3class\n",
    "count = calAccuracy_Euc(x_test_img[0:6000],S_attr_2[0:3],class_num = 3)\n",
    "print('accuracy is %0.2f%%'%(100*count/6000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 语义嵌入视觉(autoencoder)\n",
    "# epochs = 10000\n",
    "# att_input = 1000\n",
    "# img_input = 2048\n",
    "# att_hid = 1350\n",
    "\n",
    "\n",
    "# def weight_variable(shape):\n",
    "\n",
    "#     initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# def bias_variable(shape):\n",
    "\n",
    "#     initial = tf.constant(0.05, shape=shape)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # Placeholder\n",
    "# # define placeholder for inputs to network\n",
    "# att_features = tf.placeholder(tf.float32, [None, att_input])\n",
    "# #att_features = tf.placeholder(tf.float32, [None, 512])\n",
    "# visual_features = tf.placeholder(tf.float32, [None, img_input])\n",
    "\n",
    "# # # Network\n",
    "\n",
    "# # # encoder\n",
    "# W_left_a1 = weight_variable([att_input, att_hid])\n",
    "# #W_left_a1 = weight_variable([512, 1024])\n",
    "# b_left_a1 = bias_variable([att_hid])\n",
    "# left_a1 = tf.nn.relu(tf.matmul(att_features, W_left_a1) + b_left_a1)\n",
    "\n",
    "\n",
    "# W_left_a2 = weight_variable([att_hid, img_input])\n",
    "# b_left_a2 = bias_variable([img_input])\n",
    "# left_a2 = tf.nn.relu(tf.matmul(left_a1, W_left_a2) + b_left_a2)\n",
    "\n",
    "# ## decoder\n",
    "# W_left_a3 = weight_variable([img_input, att_hid])\n",
    "# #W_left_a1 = weight_variable([512, 1024])\n",
    "# b_left_a3 = bias_variable([att_hid])\n",
    "# left_a3_att = tf.nn.relu(tf.matmul(left_a2, W_left_a3) + b_left_a3)\n",
    "# left_a3_img = tf.nn.relu(tf.matmul(visual_features, W_left_a3) + b_left_a3)\n",
    "\n",
    "# W_left_a4 = weight_variable([att_hid, att_input])\n",
    "# b_left_a4 = bias_variable([att_input])\n",
    "# left_a4_att = tf.nn.relu(tf.matmul(left_a3_att, W_left_a4) + b_left_a4)\n",
    "# left_a4_img = tf.nn.relu(tf.matmul(left_a3_img, W_left_a4) + b_left_a4)\n",
    "\n",
    "# # # loss\n",
    "# loss_a = tf.losses.huber_loss(visual_features,left_a2)\n",
    "# # loss_a = wasserstein_loss(visual_features,left_a2 )\n",
    "# # loss_b = tf.losses.mean_squared_error(att_features, left_a4)\n",
    "# loss_b = tf.losses.mean_squared_error(att_features, left_a4_att)\n",
    "# loss_c = tf.losses.mean_squared_error(att_features, left_a4_img)\n",
    "# # L2 regularisation for the fully connected parameters.\n",
    "\n",
    "# regularisers_a = (tf.nn.l2_loss(W_left_a1) + tf.nn.l2_loss(b_left_a1)\n",
    "#                   + tf.nn.l2_loss(W_left_a2) + tf.nn.l2_loss(b_left_a2)\n",
    "#                  +tf.nn.l2_loss(W_left_a3) + tf.nn.l2_loss(b_left_a3)\n",
    "#                   + tf.nn.l2_loss(W_left_a4) + tf.nn.l2_loss(b_left_a4))\n",
    "\n",
    "# k = 0.9\n",
    "# loss_sum = loss_a+k*loss_b+k*loss_c+1e-3 * regularisers_a\n",
    "\n",
    "\n",
    "\n",
    "# #acc=  accuracy()\n",
    "\n",
    "# train_step = tf.train.AdamOptimizer(0.0005).minimize(loss_sum)\n",
    "\n",
    "# sess = tf.Session()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# # fig_loss = np.zeros([10])\n",
    "# # fig_acc = np.zeros([10])\n",
    "# #data_fit()\n",
    "\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     s=[]\n",
    "#     # Select a random batch of images\n",
    "#     idx = np.random.randint(0, x_train_img.shape[0], 100)\n",
    "\n",
    "#     v_feature = x_train_img[idx]\n",
    "#     s = aux_data_train[idx]\n",
    "# #     for i in range(len(idx)):\n",
    "\n",
    "# #         if idx[i] <2000:\n",
    "# #             s_attr = aux_data_train[0]\n",
    "# #             s.append(s_attr)\n",
    "# #         if 2000<= idx[i] < 4000:\n",
    "# #             s_attr = aux_data_train[2000]\n",
    "# #             s.append(s_attr)\n",
    "# #         if 4000<= idx[i] < 6000:\n",
    "# #             s_attr = aux_data_train[4000]\n",
    "# #             s.append(s_attr)\n",
    "\n",
    "# #     s_attr = np.atleast_2d(s_attr)\n",
    "\n",
    "#     _, loss_val= sess.run([train_step, loss_sum], feed_dict={visual_features: v_feature , att_features: s})\n",
    "    \n",
    "#     if epoch%100==0:\n",
    "#         print('train loss',loss_val)\n",
    "#         # 7种语义嵌入子空间\n",
    "#         s_attr_1 = np.array([aux_data_train[0],aux_data_train[2000],aux_data_train[4000]])\n",
    "#         S_attr_1= sess.run(left_a2, feed_dict={att_features: s_attr_1})\n",
    "# #         print(S_attr_1.shape)\n",
    "#         s_attr_2 = np.array([aux_data_test[0],aux_data_test[2000],aux_data_test[4000],aux_data_test[6000]])\n",
    "#         S_attr_2= sess.run(left_a2, feed_dict={att_features: s_attr_2})\n",
    "# #         print(S_attr_2.shape)\n",
    "#         count_1 = calAccuracy_Euc(x_test_img,S_attr_2,class_num = 4)\n",
    "#         count_2 = calAccuracy_Euc(x_test_img[0:6000],S_attr_2[0:3],class_num = 3)\n",
    "#         print('accuracy is %0.2f%%'%(100*count_1/8000))     \n",
    "#         print('accuracy is %0.2f%%'%(100*count_2/6000))  \n",
    "\n",
    "#    # print('accuracy:',accuracy)\n",
    "# #                 if epoch %1000 == 0:\n",
    "# #                     acc= accuracy()\n",
    "# #                     fig_loss[epoch//1000] = loss_val\n",
    "# #                     fig_acc[epoch//1000] = acc\n",
    "\n",
    "\n",
    "# # print(acc)                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 语义嵌入视觉(vae)\n",
    "# epochs = 10000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def weight_variable(shape):\n",
    "\n",
    "#     initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "# def bias_variable(shape):\n",
    "\n",
    "#     initial = tf.constant(0.05, shape=shape)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # Placeholder\n",
    "# # define placeholder for inputs to network\n",
    "# att_features = tf.placeholder(tf.float32, [None, 1000])\n",
    "# #att_features = tf.placeholder(tf.float32, [None, 512])\n",
    "# visual_features = tf.placeholder(tf.float32, [None, 2048])\n",
    "\n",
    "# # # Network\n",
    "\n",
    "# # # encoder\n",
    "# W_left_a1 = weight_variable([1000, 1560])\n",
    "# #W_left_a1 = weight_variable([512, 1024])\n",
    "# b_left_a1 = bias_variable([1560])\n",
    "# left_a1 = tf.nn.relu(tf.matmul(att_features, W_left_a1) + b_left_a1)\n",
    "\n",
    "\n",
    "# W_left_a2_mean = weight_variable([1560, 2048])\n",
    "# b_left_a2_mean = bias_variable([2048])\n",
    "# left_a2_mean = tf.nn.relu(tf.matmul(left_a1, W_left_a2_mean) + b_left_a2_mean)\n",
    "\n",
    "# W_left_a2_var = weight_variable([1560, 2048])\n",
    "# b_left_a2_var = bias_variable([2048])\n",
    "# left_a2_var = tf.nn.relu(tf.matmul(left_a1, W_left_a2_var) + b_left_a2_var)\n",
    "\n",
    "# # 重参数技巧\n",
    "# def sampling(args):\n",
    "#     z_mean, z_log_var = args\n",
    "#     epsilon = K.random_normal(shape=K.shape(z_mean))\n",
    "#     return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# # 重参数层，相当于给输入加入噪声\n",
    "# z = Lambda(sampling, output_shape=(2048,))([left_a2_mean, left_a2_var])\n",
    "\n",
    "# ## decoder\n",
    "# W_left_a3 = weight_variable([2048, 1560])\n",
    "# #W_left_a1 = weight_variable([512, 1024])\n",
    "# b_left_a3 = bias_variable([1560])\n",
    "# left_a3 = tf.nn.relu(tf.matmul(z, W_left_a3) + b_left_a3)\n",
    "\n",
    "\n",
    "# W_left_a4 = weight_variable([1560, 1000])\n",
    "# b_left_a4 = bias_variable([1000])\n",
    "# left_a4 = tf.nn.relu(tf.matmul(left_a3, W_left_a4) + b_left_a4)\n",
    "\n",
    "# # # # loss\n",
    "\n",
    "# loss_a = tf.losses.mean_squared_error(z, visual_features) \n",
    "# loss_b = tf.losses.mean_squared_error(att_features, left_a4)\n",
    "\n",
    "# # marginal_likelihood loss为y与输入数据x之间交叉墒，即解码器的损失\n",
    "# # marginal_likelihood = tf.reduce_sum(att_features * tf.log(left_a4) + (1 - att_features) * tf.log(1 - left_a4), 1)\n",
    "# # marginal_likelihood = tf.losses.mean_squared_error(att_features, left_a4)\n",
    "\n",
    "# # # KL_divergence为z与标准高斯分布之间的差距，即编码器的损失\n",
    "# # KL_divergence = 0.5 * tf.reduce_sum(tf.square(left_a2_mean) + tf.square(left_a2_var) - tf.log(1e-8 + tf.square(left_a2_var)) - 1, 1)\n",
    "# # KL_divergence = tf.reduce_mean(KL_divergence)\n",
    "\n",
    "# # # 变分下界L(x)，目标最大化\n",
    "# # ELBO = marginal_likelihood - KL_divergence\n",
    "\n",
    "# # # 令损失函数为-L(x)，目标梯度下降最小化\n",
    "# # loss_b = -ELBO\n",
    "\n",
    "# # L2 regularisation for the fully connected parameters.\n",
    "\n",
    "# regularisers_a = (tf.nn.l2_loss(W_left_a1) + tf.nn.l2_loss(b_left_a1)+ tf.nn.l2_loss(W_left_a2_mean) + tf.nn.l2_loss(b_left_a2_var)\n",
    "#                   +tf.nn.l2_loss(W_left_a2_var) + tf.nn.l2_loss(b_left_a2_var)\n",
    "#                  +tf.nn.l2_loss(W_left_a3) + tf.nn.l2_loss(b_left_a3)+ tf.nn.l2_loss(W_left_a4) + tf.nn.l2_loss(b_left_a4))\n",
    "\n",
    "\n",
    "# # loss_a = loss_a+0.5*loss_b+1e-3 * regularisers_a\n",
    "# loss_sum = loss_a+0.5*loss_b+1e-3 * regularisers_a\n",
    "\n",
    "\n",
    "\n",
    "# #acc=  accuracy()\n",
    "\n",
    "# train_step = tf.train.AdamOptimizer(0.001).minimize(loss_sum)\n",
    "\n",
    "# sess = tf.Session()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# fig_loss = np.zeros([10])\n",
    "# fig_acc = np.zeros([10])\n",
    "# #data_fit()\n",
    "\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     s=[]\n",
    "#     # Select a random batch of images\n",
    "#     idx = np.random.randint(0, x_train_img.shape[0], 100)\n",
    "\n",
    "#     v_feature = x_train_img[idx]\n",
    "#     s = aux_data_train[idx]\n",
    "# #     for i in range(len(idx)):\n",
    "\n",
    "# #         if idx[i] <2000:\n",
    "# #             s_attr = aux_data_train[0]\n",
    "# #             s.append(s_attr)\n",
    "# #         if 2000<= idx[i] < 4000:\n",
    "# #             s_attr = aux_data_train[2000]\n",
    "# #             s.append(s_attr)\n",
    "# #         if 4000<= idx[i] < 6000:\n",
    "# #             s_attr = aux_data_train[4000]\n",
    "# #             s.append(s_attr)\n",
    "\n",
    "# #     s_attr = np.atleast_2d(s_attr)\n",
    "\n",
    "#     _, loss_val= sess.run([train_step, loss_a], feed_dict={visual_features: v_feature , att_features: s})\n",
    "    \n",
    "#     if epoch%100==0:\n",
    "#         print('train loss',loss_val)\n",
    "\n",
    "#    # print('accuracy:',accuracy)\n",
    "# #                 if epoch %1000 == 0:\n",
    "# #                     acc= accuracy()\n",
    "# #                     fig_loss[epoch//1000] = loss_val\n",
    "# #                     fig_acc[epoch//1000] = acc\n",
    "\n",
    "\n",
    "# # print(acc)                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 设置超参数\n",
    "# batch_size = 100\n",
    "# original_dim_img = 2048\n",
    "# original_dim_att = 1000\n",
    "# latent_dim = 2048 # 隐变量取2维只是为了方便后面画图\n",
    "# intermediate_dim = 1560\n",
    "# epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_img = Input(shape=(original_dim_img,))\n",
    "# x_att = Input(shape=(original_dim_att,))\n",
    "# h_att = Dense(intermediate_dim, activation='relu')(x_att)\n",
    "\n",
    "# # 算p(Z|X)的均值和方差\n",
    "# z_mean = Dense(latent_dim)(h_att)\n",
    "# z_log_var = Dense(latent_dim)(h_att)\n",
    "\n",
    "# # 重参数技巧\n",
    "# def sampling(args):\n",
    "#     z_mean, z_log_var = args\n",
    "#     epsilon = K.random_normal(shape=K.shape(z_mean))\n",
    "#     return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# # 重参数层，相当于给输入加入噪声\n",
    "# z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# # 解码层，也就是生成器部分\n",
    "# decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "# decoder_mean = Dense(original_dim_att, activation='sigmoid')\n",
    "# h_decoded = decoder_h(z)\n",
    "# x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# # 建立模型\n",
    "# model = Model([x_img,x_att], x_decoded_mean)\n",
    "\n",
    "# # xent_loss是重构loss，kl_loss是KL loss\n",
    "# xent_loss = K.sum(K.binary_crossentropy(x_att, x_decoded_mean), axis=-1)\n",
    "# kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "# vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "# # 对齐loss\n",
    "# align_loss = 0.5*tf.losses.mean_squared_error(z, x_img) \n",
    "\n",
    "# # loss_sum = vae_loss + 0.5*align_loss\n",
    "\n",
    "# # add_loss是新增的方法，用于更灵活地添加各种loss\n",
    "# loss_sum = model.add_loss(vae_loss)\n",
    "# loss_sum = model.add_loss(align_loss)\n",
    "\n",
    "# opt = keras.optimizers.adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "# model.compile(optimizer=opt,metrics=[vae_loss, align_loss])\n",
    "# model.summary()\n",
    "\n",
    "# model.fit([x_train_img,aux_data_train],\n",
    "#         shuffle=True,\n",
    "#         epochs=epochs,\n",
    "#         batch_size=batch_size,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7种语义嵌入子空间\n",
    "# s_attr_1 = np.array([aux_data_train[0],aux_data_train[2000],aux_data_train[4000]])\n",
    "# #取某一层的输出为输出新建为model，采用函数模型\n",
    "# z_layer_model = Model(inputs=x_att,outputs=model.get_layer('lambda_11').output)\n",
    "# #以这个model的预测值作为输出\n",
    "# S_attr_1 = z_layer_model.predict(s_attr_1)\n",
    "# print(S_attr_1.shape)\n",
    "\n",
    "# s_attr_2 = np.array([aux_data_test[0],aux_data_test[2000],aux_data_test[4000],aux_data_test[6000]])\n",
    "# #取某一层的输出为输出新建为model，采用函数模型\n",
    "# # z_layer_model = Model(inputs=model.input,outputs=model.get_layer('lambda_2').output)\n",
    "# #以这个model的预测值作为输出\n",
    "# S_attr_2 = z_layer_model.predict(s_attr_2)\n",
    "# print(S_attr_2.shape)\n",
    "\n",
    "# # 计算欧氏距离\n",
    "# def dist(x,y):\n",
    "#     return np.sqrt(np.sum(np.square(x-y)))\n",
    "\n",
    "# # 按欧式距离计算精确度\n",
    "# def calAccuracy_Euc(latent_x,latent_c,class_num = 3):\n",
    "#     predict_list = []\n",
    "# #     latent_x = latent_x.tolist()\n",
    "# #     latent_c = latent_c.tolist()\n",
    "    \n",
    "#     if class_num == 3:\n",
    "#         for i in range(6000): \n",
    "#             distance_list = [dist(latent_x[i], latent_c[0]),\n",
    "#                              dist(latent_x[i], latent_c[1]),\n",
    "#                              dist(latent_x[i], latent_c[2])]\n",
    "#             min_index = distance_list.index(min(distance_list))\n",
    "#             predict_list.append(min_index)\n",
    "        \n",
    "#         print(predict_list)\n",
    "\n",
    "#         count = 0\n",
    "#         for i in range(0,2000):\n",
    "#             if predict_list[i]==0:\n",
    "#                 count=count+1\n",
    "#         for i in range(2000,4000):\n",
    "#             if predict_list[i]==1:\n",
    "#                 count=count+1\n",
    "#         for i in range(4000,6000):\n",
    "#             if predict_list[i]==2:\n",
    "#                 count=count+1\n",
    "\n",
    "#         return count\n",
    "    \n",
    "#     if class_num == 4:\n",
    "#         for i in range(8000):\n",
    "#             distance_list = [dist(latent_x[i], latent_c[0]),\n",
    "#                              dist(latent_x[i], latent_c[1]),\n",
    "#                              dist(latent_x[i], latent_c[2]),\n",
    "#                              dist(latent_x[i], latent_c[3])]\n",
    "#             min_index = distance_list.index(min(distance_list))\n",
    "#             predict_list.append(min_index)\n",
    "        \n",
    "#         print(predict_list)\n",
    "\n",
    "#         count = 0\n",
    "#         for i in range(0,2000):\n",
    "#             if predict_list[i]==0:\n",
    "#                 count=count+1\n",
    "#         for i in range(2000,4000):\n",
    "#             if predict_list[i]==1:\n",
    "#                 count=count+1\n",
    "#         for i in range(4000,6000):\n",
    "#             if predict_list[i]==2:\n",
    "#                 count=count+1\n",
    "#         for i in range(6000,8000):\n",
    "#             if predict_list[i]==3:\n",
    "#                 count=count+1\n",
    "#         return count\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 语义嵌入视觉\n",
    "# epochs = 10000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def weight_variable(shape):\n",
    "\n",
    "#     initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "# def bias_variable(shape):\n",
    "\n",
    "#     initial = tf.constant(0.05, shape=shape)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # Placeholder\n",
    "# # define placeholder for inputs to network\n",
    "# att_features = tf.placeholder(tf.float32, [None, 1000])\n",
    "# #att_features = tf.placeholder(tf.float32, [None, 512])\n",
    "# visual_features = tf.placeholder(tf.float32, [None, 2048])\n",
    "\n",
    "# # # Network\n",
    "\n",
    "\n",
    "# W_left_a1 = weight_variable([1000, 1024])\n",
    "# #W_left_a1 = weight_variable([512, 1024])\n",
    "# b_left_a1 = bias_variable([1024])\n",
    "# left_a1 = tf.nn.relu(tf.matmul(att_features, W_left_a1) + b_left_a1)\n",
    "\n",
    "\n",
    "# W_left_a2 = weight_variable([1024, 2048])\n",
    "# b_left_a2 = bias_variable([2048])\n",
    "# left_a2 = tf.nn.relu(tf.matmul(left_a1, W_left_a2) + b_left_a2)\n",
    "\n",
    "# # # loss\n",
    "\n",
    "# loss_a = tf.reduce_mean(tf.square(left_a2 - visual_features))    \n",
    "\n",
    "# # L2 regularisation for the fully connected parameters.\n",
    "\n",
    "# regularisers_a = (tf.nn.l2_loss(W_left_a1) + tf.nn.l2_loss(b_left_a1)\n",
    "#                     + tf.nn.l2_loss(W_left_a2) + tf.nn.l2_loss(b_left_a2))\n",
    "\n",
    "\n",
    "# loss_a += 1e-3 * regularisers_a\n",
    "\n",
    "\n",
    "\n",
    "# #acc=  accuracy()\n",
    "\n",
    "# train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_a)\n",
    "\n",
    "# sess = tf.Session()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# fig_loss = np.zeros([10])\n",
    "# fig_acc = np.zeros([10])\n",
    "# #data_fit()\n",
    "\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     s=[]\n",
    "#     # Select a random batch of images\n",
    "#     idx = np.random.randint(0, x_train_img.shape[0], 100)\n",
    "\n",
    "#     v_feature = x_train_img[idx]\n",
    "#     s = aux_data_train[idx]\n",
    "# #     for i in range(len(idx)):\n",
    "\n",
    "# #         if idx[i] <2000:\n",
    "# #             s_attr = aux_data_train[0]\n",
    "# #             s.append(s_attr)\n",
    "# #         if 2000<= idx[i] < 4000:\n",
    "# #             s_attr = aux_data_train[2000]\n",
    "# #             s.append(s_attr)\n",
    "# #         if 4000<= idx[i] < 6000:\n",
    "# #             s_attr = aux_data_train[4000]\n",
    "# #             s.append(s_attr)\n",
    "\n",
    "# #     s_attr = np.atleast_2d(s_attr)\n",
    "\n",
    "#     _, loss_val= sess.run([train_step, loss_a], feed_dict={visual_features: v_feature , att_features: s})\n",
    "    \n",
    "#     if epoch%100==0:\n",
    "#         print('train loss',loss_val)\n",
    "\n",
    "#    # print('accuracy:',accuracy)\n",
    "# #                 if epoch %1000 == 0:\n",
    "# #                     acc= accuracy()\n",
    "# #                     fig_loss[epoch//1000] = loss_val\n",
    "# #                     fig_acc[epoch//1000] = acc\n",
    "\n",
    "\n",
    "# # print(acc)                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 语义与视觉嵌入公共子空间\n",
    "# epochs = 10000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def weight_variable(shape):\n",
    "\n",
    "#     initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "# def bias_variable(shape):\n",
    "\n",
    "#     initial = tf.constant(0.05, shape=shape)\n",
    "#     return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # Placeholder\n",
    "# # define placeholder for inputs to network\n",
    "# att_features = tf.placeholder(tf.float32, [None, 1000])\n",
    "# #att_features = tf.placeholder(tf.float32, [None, 512])\n",
    "# visual_features = tf.placeholder(tf.float32, [None, 512])\n",
    "\n",
    "# # # Network\n",
    "\n",
    "\n",
    "# W_left_a1 = weight_variable([1000, 1024])\n",
    "# #W_left_a1 = weight_variable([512, 1024])\n",
    "# b_left_a1 = bias_variable([1024])\n",
    "# left_a1 = tf.nn.relu(tf.matmul(att_features, W_left_a1) + b_left_a1)\n",
    "\n",
    "\n",
    "# W_left_a2 = weight_variable([1024, 2048])\n",
    "# b_left_a2 = bias_variable([2048])\n",
    "# left_a2 = tf.nn.relu(tf.matmul(left_a1, W_left_a2) + b_left_a2)\n",
    "\n",
    "# W_left_b1 = weight_variable([512, 1024])\n",
    "# #W_left_a1 = weight_variable([512, 1024])\n",
    "# b_left_b1 = bias_variable([1024])\n",
    "# left_b1 = tf.nn.relu(tf.matmul(visual_features, W_left_b1) + b_left_b1)\n",
    "\n",
    "\n",
    "# W_left_b2 = weight_variable([1024, 2048])\n",
    "# b_left_b2 = bias_variable([2048])\n",
    "# left_b2 = tf.nn.relu(tf.matmul(left_b1, W_left_b2) + b_left_b2)\n",
    "\n",
    "\n",
    "\n",
    "# # # loss\n",
    "\n",
    "# loss_a = tf.reduce_mean(tf.square(left_a2 - left_b2))    \n",
    "\n",
    "# # L2 regularisation for the fully connected parameters.\n",
    "\n",
    "# regularisers_a = (tf.nn.l2_loss(W_left_a1) + tf.nn.l2_loss(b_left_a1)+ tf.nn.l2_loss(W_left_a2) + tf.nn.l2_loss(b_left_a2)\n",
    "#                  +tf.nn.l2_loss(W_left_b1) + tf.nn.l2_loss(b_left_b1)+ tf.nn.l2_loss(W_left_b2) + tf.nn.l2_loss(b_left_b2))\n",
    "\n",
    "\n",
    "# loss_a += 1e-3 * regularisers_a\n",
    "\n",
    "\n",
    "\n",
    "# #acc=  accuracy()\n",
    "\n",
    "# train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_a)\n",
    "\n",
    "# sess = tf.Session()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# fig_loss = np.zeros([10])\n",
    "# fig_acc = np.zeros([10])\n",
    "# #data_fit()\n",
    "\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     s=[]\n",
    "#     # Select a random batch of images\n",
    "#     idx = np.random.randint(0, x_train_img.shape[0], 100)\n",
    "\n",
    "#     v_feature = x_train_img[idx]\n",
    "#     s = aux_data_train[idx]\n",
    "# #     for i in range(len(idx)):\n",
    "\n",
    "# #         if idx[i] <2000:\n",
    "# #             s_attr = aux_data_train[0]\n",
    "# #             s.append(s_attr)\n",
    "# #         if 2000<= idx[i] < 4000:\n",
    "# #             s_attr = aux_data_train[2000]\n",
    "# #             s.append(s_attr)\n",
    "# #         if 4000<= idx[i] < 6000:\n",
    "# #             s_attr = aux_data_train[4000]\n",
    "# #             s.append(s_attr)\n",
    "\n",
    "# #     s_attr = np.atleast_2d(s_attr)\n",
    "\n",
    "#     _, loss_val= sess.run([train_step, loss_a], feed_dict={visual_features: v_feature , att_features: s})\n",
    "    \n",
    "#     if epoch%100==0:\n",
    "#         print('train loss',loss_val)\n",
    "\n",
    "#    # print('accuracy:',accuracy)\n",
    "# #                 if epoch %1000 == 0:\n",
    "# #                     acc= accuracy()\n",
    "# #                     fig_loss[epoch//1000] = loss_val\n",
    "# #                     fig_acc[epoch//1000] = acc\n",
    "\n",
    "\n",
    "# # print(acc)                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 按余弦相似度分类\n",
    "# def cosine_classify(_latent_visual,_latent_semantic):\n",
    "#     predict_list = []\n",
    "#     latent_visual = _latent_visual.tolist()\n",
    "#     latent_semantic = _latent_semantic.tolist()\n",
    "#     if _latent_semantic.shape[0] == 3:\n",
    "#         for i in range(len(latent_visual)):\n",
    "#             cosine_list = [cosine_similarity(latent_visual[i], latent_semantic[0]),\n",
    "#                            cosine_similarity(latent_visual[i], latent_semantic[1]),\n",
    "#                            cosine_similarity(latent_visual[i], latent_semantic[2]),\n",
    "#     #                        cosine_similarity(latent_visual[i], latent_semantic[3])\n",
    "#                           ]\n",
    "#             max_index = cosine_list.index(max(cosine_list))\n",
    "#             predict_list.append(max_index)\n",
    "#     else:\n",
    "#         for i in range(len(latent_visual)):\n",
    "#             cosine_list = [cosine_similarity(latent_visual[i], latent_semantic[0]),\n",
    "#                            cosine_similarity(latent_visual[i], latent_semantic[1]),\n",
    "#                            cosine_similarity(latent_visual[i], latent_semantic[2]),\n",
    "#                            cosine_similarity(latent_visual[i], latent_semantic[3])\n",
    "#                           ]\n",
    "#             max_index = cosine_list.index(max(cosine_list))\n",
    "#             predict_list.append(max_index)\n",
    "            \n",
    "#     return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算精确度\n",
    "# def calAccuracy(latent_x,latent_c):\n",
    "    \n",
    "#     predict_list = cosine_classify(latent_x,latent_c)\n",
    "    \n",
    "#     print(predict_list)\n",
    "\n",
    "#     count = 0\n",
    "#     for i in range(0,2000):\n",
    "#         if predict_list[i]==0:\n",
    "#             count=count+1\n",
    "#     for i in range(2000,4000):\n",
    "#         if predict_list[i]==1:\n",
    "#             count=count+1\n",
    "#     for i in range(4000,6000):\n",
    "#         if predict_list[i]==2:\n",
    "#             count=count+1\n",
    "#     if len(predict_list)>6000:\n",
    "#         for i in range(6000,8000):\n",
    "#             if predict_list[i]==3:\n",
    "#                 count=count+1\n",
    "            \n",
    "\n",
    "#     return count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算欧氏距离\n",
    "# def calEuclideanDistance(vec1,vec2):  \n",
    "#     dist = np.sqrt(np.sum(np.square(vec1 - vec2)))  \n",
    "#     return dist  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 视觉向量嵌入子空间\n",
    "# V_img_1= sess.run(left_b2, feed_dict={visual_features: x_train_img})\n",
    "# print(V_img_1.shape)\n",
    "\n",
    "\n",
    "# V_img_2= sess.run(left_b2, feed_dict={visual_features: x_test_img})\n",
    "# print(V_img_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
